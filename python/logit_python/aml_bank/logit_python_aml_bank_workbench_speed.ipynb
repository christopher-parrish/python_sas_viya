{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For financial services organizations, model development for fraud detection and for surfacing potentially anti-money laundering activities are an area of increasing interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bespoke models may be used by banks to replace rules-based scenarios or other fraud detection activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This use case models bank account holder activity to determine if the probability of money launding event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Developing Python / Notebook to create dataframe and set up training and testing partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analytics base table (aml_bank_prep) has already gone through ETL process and is prepped for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Engines are built into Workbench to access and process data in external data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "account_id                       int64\n",
      "num_transactions               float64\n",
      "credit_score                   float64\n",
      "marital_status_single            int64\n",
      "marital_status_married           int64\n",
      "marital_status_divorced          int64\n",
      "analytic_partition               int64\n",
      "ml_indicator                     int64\n",
      "checking_only_indicator          int64\n",
      "prior_ctr_indicator              int64\n",
      "address_change_2x_indicator      int64\n",
      "cross_border_trx_indicator       int64\n",
      "in_person_contact_indicator      int64\n",
      "linkedin_indicator               int64\n",
      "atm_deposit_indicator            int64\n",
      "trx_10ksum_indicator             int64\n",
      "common_merchant_indicator        int64\n",
      "direct_deposit_indicator         int64\n",
      "citizenship_country_risk         int64\n",
      "occupation_risk                  int64\n",
      "num_acctbal_chgs_gt2000        float64\n",
      "distance_to_employer           float64\n",
      "distance_to_bank               float64\n",
      "income                         float64\n",
      "primary_transfer_cash            int64\n",
      "primary_transfer_check           int64\n",
      "primary_transfer_wire            int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### Create Dataframe ###\n",
    "########################\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "workspace_dir = \"/workspaces/chris_parrish/__fraud_detection_use_case__/\"\n",
    "data_table = \"aml_bank_prep_synthetic.csv\"\n",
    "\n",
    "dm_inputdf = pd.read_csv(Path(workspace_dir) / data_table, header=0)\n",
    "print(dm_inputdf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### Model Parameters ###\n",
    "########################\n",
    "\n",
    "### import python libraries\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "### model manager information\n",
    "metadata_output_dir = 'outputs'\n",
    "model_name = 'logit_python_aml_bank_workbench'\n",
    "project_name = 'Anti-Money Laundering'\n",
    "description = 'Logistic Regression'\n",
    "model_type = 'logistic_regression'\n",
    "model_function = 'Classification'\n",
    "predict_syntax = 'predict_proba'\n",
    "\n",
    "### define macro variables for model\n",
    "dm_dec_target = 'ml_indicator'\n",
    "dm_partitionvar = 'analytic_partition'\n",
    "create_new_partition = 'no' # 'yes', 'no'\n",
    "dm_key = 'account_id' \n",
    "dm_classtarget_level = ['0', '1']\n",
    "dm_partition_validate_val, dm_partition_train_val, dm_partition_test_val = [0, 1, 2]\n",
    "dm_partition_validate_perc, dm_partition_train_perc, dm_partition_test_perc = [0.3, 0.6, 0.1]\n",
    "\n",
    "### create list of regressors\n",
    "keep_predictors = [\n",
    "    'marital_status_single',\n",
    "    'checking_only_indicator',\n",
    "    'prior_ctr_indicator',\n",
    "    'address_change_2x_indicator',\n",
    "    'cross_border_trx_indicator',\n",
    "    'in_person_contact_indicator',\n",
    "    'linkedin_indicator',\n",
    "    'citizenship_country_risk',\n",
    "    'distance_to_employer',\n",
    "    'distance_to_bank'\n",
    "    ]\n",
    "#rejected_predictors = []\n",
    "\n",
    "### mlflow\n",
    "use_mlflow = 'no' # 'yes', 'no'\n",
    "mlflow_run_to_use = 0\n",
    "mlflow_class_labels =['TENSOR']\n",
    "mlflow_predict_syntax = 'predict'\n",
    "\n",
    "### var to consider in bias assessment\n",
    "bias_vars = ['marital_status_single']\n",
    "\n",
    "### var to consider in partial dependency\n",
    "pd_var1 = 'distance_to_employer'\n",
    "pd_var2 = 'distance_to_bank'\n",
    "\n",
    "### create partition column, if not already in dataset\n",
    "if create_new_partition == 'yes':\n",
    "    dm_inputdf = shuffle(dm_inputdf)\n",
    "    dm_inputdf.reset_index(inplace=True, drop=True)\n",
    "    validate_rows = round(len(dm_inputdf)*dm_partition_validate_perc)\n",
    "    train_rows = round(len(dm_inputdf)*dm_partition_train_perc) + validate_rows\n",
    "    test_rows = len(dm_inputdf)-train_rows\n",
    "    dm_inputdf.loc[0:validate_rows,dm_partitionvar] = dm_partition_validate_val\n",
    "    dm_inputdf.loc[validate_rows:train_rows,dm_partitionvar] = dm_partition_train_val\n",
    "    dm_inputdf.loc[train_rows:,dm_partitionvar] = dm_partition_test_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marital_status_single', 'checking_only_indicator', 'prior_ctr_indicator', 'address_change_2x_indicator', 'cross_border_trx_indicator', 'in_person_contact_indicator', 'linkedin_indicator', 'citizenship_country_risk', 'distance_to_employer', 'distance_to_bank']\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "### Final Modeling Columns ###\n",
    "##############################\n",
    "\n",
    "### create list of model variables\n",
    "dm_input = list(dm_inputdf.columns.values)\n",
    "macro_vars = (dm_dec_target + ' ' + dm_partitionvar + ' ' + dm_key).split()\n",
    "rejected_predictors = [i for i in dm_input if i not in keep_predictors]\n",
    "rejected_vars = rejected_predictors # + macro_vars (include macro_vars if rejected_predictors are explicitly listed - not contra keep_predictors)\n",
    "for i in rejected_vars:\n",
    "    dm_input.remove(i)\n",
    "print(dm_input)\n",
    "\n",
    "### create prediction variables\n",
    "dm_predictionvar = [str('P_') + dm_dec_target + dm_classtarget_level[0], str('P_') + dm_dec_target + dm_classtarget_level[1]]\n",
    "dm_classtarget_intovar = str('I_') + dm_dec_target\n",
    "\n",
    "##################\n",
    "### Data Split ###\n",
    "##################\n",
    "\n",
    "### create train, test, validate datasets using existing partition column\n",
    "dm_traindf = dm_inputdf[dm_inputdf[dm_partitionvar] == dm_partition_train_val]\n",
    "X_train = dm_traindf.loc[:, dm_input]\n",
    "y_train = dm_traindf[dm_dec_target]\n",
    "dm_testdf = dm_inputdf.loc[(dm_inputdf[dm_partitionvar] == dm_partition_test_val)]\n",
    "X_test = dm_testdf.loc[:, dm_input]\n",
    "y_test = dm_testdf[dm_dec_target]\n",
    "dm_validdf = dm_inputdf.loc[(dm_inputdf[dm_partitionvar] == dm_partition_validate_val)]\n",
    "X_valid = dm_validdf.loc[:, dm_input]\n",
    "y_valid = dm_validdf[dm_dec_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350000, 27)\n"
     ]
    }
   ],
   "source": [
    "print(dm_inputdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Train model using sklearn and with sasviya.ml API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_train: 0.97523340310508\n",
      "score_test: 0.9626098443512722\n",
      "score_valid: 0.9728206256998211\n",
      "Time to complete model fit with Python: 23.644811630249023\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "### Python ###\n",
    "##############\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "### estimate & fit model\n",
    "dm_model = GradientBoostingClassifier()\n",
    "\n",
    "start = time()\n",
    "dm_model.fit(X_train, y_train)\n",
    "finish = time()\n",
    "\n",
    "print('score_train:', dm_model.score(X_train, y_train))\n",
    "print('score_test:', dm_model.score(X_test, y_test))\n",
    "print('score_valid:', dm_model.score(X_valid, y_valid))\n",
    "\n",
    "time_to_complete = finish-start\n",
    "print(\"Time to complete model fit with Python:\", time_to_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_train: 0.9768803960377243\n",
      "score_test: 0.9628108666934696\n",
      "score_valid: 0.9726579322621087\n",
      "Time to complete model fit with Python API: 1.1506941318511963\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "### Python API ###\n",
    "##################\n",
    "\n",
    "from sasviya.ml.tree import GradientBoostingClassifier\n",
    "\n",
    "### estimate & fit model\n",
    "dm_model = GradientBoostingClassifier()\n",
    "\n",
    "start = time()\n",
    "dm_model.fit(X_train, y_train)\n",
    "finish = time()\n",
    "\n",
    "print('score_train:', dm_model.score(X_train, y_train))\n",
    "print('score_test:', dm_model.score(X_test, y_test))\n",
    "print('score_valid:', dm_model.score(X_valid, y_valid))\n",
    "\n",
    "time_to_complete = finish-start\n",
    "print(\"Time to complete model fit with Python API:\", time_to_complete)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workbench Python",
   "language": "python",
   "name": "workbench_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
